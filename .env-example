# Docker/Ollama Configuration
OLLAMA_URL=http://localhost:11434/engines/llama.cpp/v1/chat/completions
# OLLAMA_URL=http://localhost:11434/api/generate
AI_MODEL=ai/deepseek-coder:6.7b

# Tuning
CONTEXT_WINDOW=8000
# If a file diff exceeds 3000 chars, we split it into chunks (hunks)
CHUNK_LIMIT=3000